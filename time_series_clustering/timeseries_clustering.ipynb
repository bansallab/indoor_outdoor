{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network-based timeseries clustering\n",
    "#### Approach: calculate correlations between county timeseries; represent correlations as a county network, with dropping low weight edges; and run community structure detection on the network to identify groups of nodes that are more similar to each out in terms of dynamics\n",
    "\n",
    "#### Author: Shweta Bansal\n",
    "#### Started Date: July 12, 2021\n",
    "#### Updated: Dec 23, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import general datasci libraries\n",
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd\n",
    "\n",
    "# import functions for the time-series clustering method\n",
    "import epi_geo_functions as fegf\n",
    "\n",
    "# run hierarchical clustering\n",
    "import scipy.cluster.hierarchy as hac\n",
    "from scipy.cluster.hierarchy import fcluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# CHANGE PARAMETERS HERE FOR REST OF ANALYSIS\n",
    "\n",
    "year_of_analysis = [2018,2019] #baseline years only\n",
    "\n",
    "years_label = \"_\".join([str(y) for y in year_of_analysis]\n",
    "\n",
    "filename = years_label+'Dec262022'\n",
    "\n",
    "threshold = 4 # absolute threshold for mean centered indoor activity measure\n",
    "\n",
    "corr_percentile = 90 # percentile for the minimum time series correlation between counties (90 = default)\n",
    "\n",
    "num_bootstrap = 25 # number of bootstrap networks over which to do analysis\n",
    "\n",
    "drop_small = 10 # drop small communities of at most this many counties\n",
    "\n",
    "contiguity_threshold = 2 # merge any island communities with neighboring cluster\n",
    "\n",
    "sandbox = False # make true if shweta debugging\n",
    "\n",
    "rolling = True\n",
    "\n",
    "num_weeks = 4 # number of weeks over which to do rolling mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>county</th>\n",
       "      <th>indoor_activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-31 00:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-07 00:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-14 00:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-21 00:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>1.208592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-28 00:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>1.086861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  county  indoor_activity\n",
       "0 2017-12-31 00:00:00+00:00    1001              NaN\n",
       "1 2018-01-07 00:00:00+00:00    1001              NaN\n",
       "2 2018-01-14 00:00:00+00:00    1001              NaN\n",
       "3 2018-01-21 00:00:00+00:00    1001         1.208592\n",
       "4 2018-01-28 00:00:00+00:00    1001         1.086861"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################################\n",
    "# LOAD DATA & CLEAN\n",
    "\n",
    "# Load indoor/outdoor data for all years, clean up, take rolling mean\n",
    "\n",
    "df_sm = pd.read_csv(\"indoor_activity_data/indoor_activity_2018_2020.csv\") # udpated file name 12/23/22 (but this is not centered)\n",
    "\n",
    "# fix date format\n",
    "df_sm['date'] = pd.to_datetime(df_sm.date, format='%Y-%m-%d')\n",
    "\n",
    "# drop counties not needed\n",
    "df_sm['state'] = (df_sm.county.astype(int)/1000).astype(int)\n",
    "df_sm = df_sm[df_sm.state < 57] # no territories\n",
    "df_sm = df_sm[df_sm.county > 999]\n",
    "\n",
    "# if there are multiple values for any county, date pair, take mean\n",
    "df_sm = df_sm.groupby(['county', 'date'])['indoor_activity'].mean().reset_index()\n",
    "\n",
    "# take rolling mean\n",
    "# number of weeks to roll defined above\n",
    "# smoothing doesn't affect clustering results, but makes time series plots cleaner\n",
    "if rolling:\n",
    "    ct_list = list(df_sm.county.unique())\n",
    "    dfn = pd.DataFrame()\n",
    "    for ct in ct_list:\n",
    "        dfx = df_sm[df_sm.county==ct]\n",
    "\n",
    "        # rolling average of time series\n",
    "        dfx = dfx.sort_values(by='date')\n",
    "        dfx = dfx[['date', 'indoor_activity']]\n",
    "        dfx = dfx.set_index('date')\n",
    "        dfx = dfx.rolling(num_weeks).mean()\n",
    "        dfx = dfx.reset_index()\n",
    "        dfx['county'] = ct\n",
    "\n",
    "        dfn = pd.concat([dfn, dfx], ignore_index=True)\n",
    "    df_sm = dfn.copy()\n",
    "    \n",
    "df_sm = df_sm[['date', 'county', 'indoor_activity']]\n",
    "\n",
    "df_sm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PREPARE TIMESERIES DATA FOR CLUSTERING\n",
    "\n",
    "df_time = df_sm.copy()\n",
    "\n",
    "# only keep data for years of interest\n",
    "df_time['year'] = df_time.date.dt.year\n",
    "df_time = df_time[df_time.year.isin(year_of_analysis)]\n",
    "df_time = df_time[['county', 'r', 'date']]\n",
    "\n",
    "# convert to long to wide format where rows = weeks, columns = fips\n",
    "df_matrix = df_time.pivot(index = 'date', columns='county', values='r').reset_index(drop=True)\n",
    "\n",
    "# keep unnormalized copy of df_matrix\n",
    "df_matrix_unnorm = df_matrix.copy()\n",
    "\n",
    "# z-normalize time series\n",
    "m = df_matrix.mean(axis=0) # take mean for each col (i.e. for each county)\n",
    "s = df_matrix.std(axis=0) # take std for each col\n",
    "df_matrix = df_matrix.sub(m, axis=1) # subtract county mean from county time series\n",
    "df_matrix = df_matrix.div(s, axis=1) # divide county time series by county stdev\n",
    "\n",
    "# clean up dataframe by making all Nans 0    \n",
    "df_matrix = df_matrix.fillna(0)\n",
    "\n",
    "df_matrix.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# CREATE CORRELATION MATRIX\n",
    "\n",
    "# get correlation matrix between timeseries\n",
    "corr_df = fegf.calc_timeseries_correlations(df_matrix)\n",
    "corr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# CREATE NETWORK\n",
    "                                            \n",
    "# create network from correlation matrix\n",
    "threshold = np.percentile(corr_df.values.tolist()[0], corr_percentile)\n",
    "network = fegf.create_network(corr_df, threshold, df_sm, False)\n",
    "                                            \n",
    "# create nearest neighbor network\n",
    "G_nn = fegf.make_nn_network(\"./\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# PERFORM TIME SERIES CLUSTERING THROUGH LOUVAIN COMMUNITY STRUCTURE\n",
    "\n",
    "# run community structure detection using Louvain algo\n",
    "part, dftemp = fegf.comm_struct_robustness(network, G_nn, 'louvain_igraph', num_bootstrap)\n",
    "part_df = fegf.reduce_num_communities(part, drop_small) # drop communities that are smaller than dropsmall of number of counties\n",
    "\n",
    "# save to file\n",
    "part_df.to_csv(\"community_structure_\"+filename+'.csv', index=False)\n",
    "\n",
    "num_clusters = len(part_df.modularity_class.unique())\n",
    "\n",
    "# output number of clusters and sizes of each cluster\n",
    "print(part_df.modularity_class.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# POST PROCESS RESULTS\n",
    "\n",
    "# make community structure feasible\n",
    "part_nonan_df = fegf.fill_in_nans(G_nn, part_df, 0.15)\n",
    "part_contig_df = fegf.increase_spatial_contiguity(G_nn, part_nonan_df, contiguity_threshold, 0.15)\n",
    "\n",
    "part_nonan_df.to_csv(\"community_structure_nonan_\"+filename+'.csv', index=False)\n",
    "part_contig_df.to_csv(\"community_structure_feasible_\"+filename+'.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# OUTPUT RESULTS\n",
    "\n",
    "# plot time series (non-zscored data)\n",
    "fegf.plot_timeseries(df_matrix_unnorm, part_df, filename+'_non_zscore', year_of_analysis, num_clusters, [0.25,2])\n",
    "\n",
    "# plot time series (z-normalized)\n",
    "#fegf.plot_timeseries(df_matrix, part_df, filename+'_zscore',  year_of_analysis, num_clusters, [-3,3])\n",
    "\n",
    "# make map\n",
    "fegf.make_module_map(part_df,filename)\n",
    "\n",
    "# make feasible map\n",
    "fegf.make_module_map(part_contig_df,filename+'_feasible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# COMPARE RESULTS TO HIERARCHICAL CLUSTERING\n",
    "\n",
    "importlib.reload(fegf)\n",
    "\n",
    "# prepare matrix\n",
    "df_matrix_hc = df_matrix.fillna(0).transpose() # needs to be county x week\n",
    "df_matrix_hc = df_matrix_hc.loc[~(df_matrix_hc==0).all(axis=1)] # remove rows with all 0s\n",
    "list_nodes = list(df_matrix_hc.index)\n",
    "\n",
    "part_hier_clust = {} # dictionary of dataframes\n",
    "for num_clusters in range(2,5): # vary number of clusters in partition\n",
    "    \n",
    "    # set up the time series linkage matrix for clustering\n",
    "    Z = hac.linkage(df_matrix_hc, method='ward', metric='euclidean')\n",
    "\n",
    "    # do time series clustering\n",
    "    results = fcluster(Z, t=num_clusters, criterion='maxclust')\n",
    "\n",
    "    # the results just tell you which partition each node (animal) is in, so this attaches the node ids to the cluster ids\n",
    "    partition = dict(zip(list_nodes, results))\n",
    "    \n",
    "    # reduce small communities\n",
    "    part_hier_clust[num_clusters] = fegf.reduce_num_communities(partition, 5)\n",
    "    \n",
    "    # output module number and sizes\n",
    "    print(part_hier_clust[num_clusters].modularity_class.value_counts())\n",
    "    \n",
    "\n",
    "#############################################\n",
    "# For num_clusters = 3, output results and quantify similartity to network clustering partition\n",
    "num_clusters = 3\n",
    "\n",
    "# relabel partitions to match the ordering of the main results\n",
    "part_hier_clust[num_clusters] = fegf.relabel_clusters(part_hier_clust[num_clusters])\n",
    "\n",
    "# output partition\n",
    "part_hier_clust[num_clusters].to_csv(\"community_structure_hierclust_\"+filename+'.csv', index=False)\n",
    "\n",
    "# plot time series (non-zscored data)\n",
    "fegf.plot_timeseries(df_matrix_unnorm, part_hier_clust[num_clusters], filename+'_hierclust_'+str(num_clusters), year_of_analysis, num_clusters, [0.25,2])\n",
    "\n",
    "# make map\n",
    "fegf.make_module_map(part_hier_clust[num_clusters],filename+'_hierclust_'+str(num_clusters))\n",
    "\n",
    "# calculate mutual information on two partitions\n",
    "part_network = pd.read_csv('community_structure_[2018, 2019]_Dec2022_corr90.csv') # this is the 90th percentile correlation network's comm struct results\n",
    "df3 = pd.merge(part_network, part_hier_clust[num_clusters], on='node', how='inner') # merge two partitions by node\n",
    "nmi = fegf.normalized_mutual_info_score(df3[\"modularity_class_x\"], df3[\"modularity_class_y\"])\n",
    "print(nmi)\n",
    "\n",
    "numlist = len(list(df3.modularity_class_x))\n",
    "match = sum([1 for a,b in zip(df3.modularity_class_x, df3.modularity_class_y) if a==b])/float(numlist)\n",
    "print(match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
